{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05005548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/week-6/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f056051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [01:02<00:00, 15.71s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model and tokenizer (in half precision)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c93dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare prompt\n",
    "msgs = [{'role': 'user', 'content': 'What is the Mandela Effect?'}]\n",
    "\n",
    "# Tokenize and move input to GPU\n",
    "inputs = tokenizer.apply_chat_template(msgs, tokenize=True, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5baadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate response on GPU\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85efa9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "\n",
      "What is the Mandela Effect?assistant\n",
      "\n",
      "The Mandela Effect is a phenomenon where a group of people remember an event or detail differently than what is recorded in history books or other sources. The term was coined in 2010 by the paranormal consultant Fiona Broome, who wrote\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
